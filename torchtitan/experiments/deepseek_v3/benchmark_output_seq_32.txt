Benchmarking MOE with:
  - 16 total experts across 4 GPUs
  - 2 layers
  - Batch size: 4, Sequence length: 32
Creating model stage 0 of 1
Creating model stage 0 of 1
Creating model stage 0 of 1
Creating model stage 0 of 1
Rank 0: MoE.__init__ DEBUG: expert 0, gate_proj.weight is a Tensor right after creation. Type: <class 'torch.nn.parameter.Parameter'>
Rank 2: MoE.__init__ DEBUG: expert 8, gate_proj.weight is a Tensor right after creation. Type: <class 'torch.nn.parameter.Parameter'>
Rank 3: MoE.__init__ DEBUG: expert 12, gate_proj.weight is a Tensor right after creation. Type: <class 'torch.nn.parameter.Parameter'>
Rank 1: MoE.__init__ DEBUG: expert 4, gate_proj.weight is a Tensor right after creation. Type: <class 'torch.nn.parameter.Parameter'>

STDERR:
[W515 16:26:27.893898241 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [localhost]:33791 (errno: 97 - Address family not supported by protocol).
[W515 16:26:27.895216007 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [localhost]:33791 (errno: 97 - Address family not supported by protocol).
[W515 16:26:28.188769906 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [holygpu8a16304.rc.fas.harvard.edu]:40801 (errno: 97 - Address family not supported by protocol).
[W515 16:26:53.311700430 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [holygpu8a16304.rc.fas.harvard.edu]:40801 (errno: 97 - Address family not supported by protocol).
[W515 16:26:53.323416501 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [holygpu8a16304.rc.fas.harvard.edu]:40801 (errno: 97 - Address family not supported by protocol).
[W515 16:26:53.325344596 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [holygpu8a16304.rc.fas.harvard.edu]:40801 (errno: 97 - Address family not supported by protocol).
[W515 16:26:53.326077034 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [holygpu8a16304.rc.fas.harvard.edu]:40801 (errno: 97 - Address family not supported by protocol).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 159, in <module>
[rank0]:     benchmark_moe_implementations(args)
[rank0]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 55, in benchmark_moe_implementations
[rank0]:     output = vanilla_model(x_vanilla)
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1591, in forward
[rank0]:     hidden_states = self.model(
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1538, in forward
[rank0]:     self.embed_tokens(tokens) if self.embed_tokens is not None else tokens
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 190, in forward
[rank0]:     return F.embedding(
[rank0]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/functional.py", line 2551, in embedding
[rank0]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
[rank0]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 159, in <module>
[rank2]:     benchmark_moe_implementations(args)
[rank2]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 55, in benchmark_moe_implementations
[rank2]:     output = vanilla_model(x_vanilla)
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1591, in forward
[rank2]:     hidden_states = self.model(
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1538, in forward
[rank2]:     self.embed_tokens(tokens) if self.embed_tokens is not None else tokens
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 190, in forward
[rank2]:     return F.embedding(
[rank2]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/functional.py", line 2551, in embedding
[rank2]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
[rank2]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:2! (when checking argument for argument index in method wrapper_CUDA__index_select)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 159, in <module>
[rank3]:     benchmark_moe_implementations(args)
[rank3]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 55, in benchmark_moe_implementations
[rank3]:     output = vanilla_model(x_vanilla)
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1591, in forward
[rank3]:     hidden_states = self.model(
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1538, in forward
[rank3]:     self.embed_tokens(tokens) if self.embed_tokens is not None else tokens
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 190, in forward
[rank3]:     return F.embedding(
[rank3]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/functional.py", line 2551, in embedding
[rank3]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
[rank3]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:3! (when checking argument for argument index in method wrapper_CUDA__index_select)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 159, in <module>
[rank1]:     benchmark_moe_implementations(args)
[rank1]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/benchmark_moe_overlap.py", line 55, in benchmark_moe_implementations
[rank1]:     output = vanilla_model(x_vanilla)
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1591, in forward
[rank1]:     hidden_states = self.model(
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/n/netscratch/idreos_lab/Lab/pseth/dv2/torchtitan/torchtitan/experiments/deepseek_v3/model.py", line 1538, in forward
[rank1]:     self.embed_tokens(tokens) if self.embed_tokens is not None else tokens
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 190, in forward
[rank1]:     return F.embedding(
[rank1]:   File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/nn/functional.py", line 2551, in embedding
[rank1]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
[rank1]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)
[rank0]:[W515 16:27:04.910763766 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W515 16:27:04.990599731 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W515 16:27:04.010359371 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W515 16:27:05.055151606 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0515 16:27:05.519000 3368096 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3368113 closing signal SIGTERM
W0515 16:27:05.519000 3368096 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3368114 closing signal SIGTERM
W0515 16:27:05.519000 3368096 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3368115 closing signal SIGTERM
E0515 16:27:05.997000 3368096 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 3368112) of binary: /n/home06/pseth/.conda/envs/dv2/bin/python3.10
Traceback (most recent call last):
  File "/n/home06/pseth/.conda/envs/dv2/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/n/home06/pseth/.conda/envs/dv2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark_moe_overlap.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-15_16:27:05
  host      : holygpu8a16304.rc.fas.harvard.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3368112)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
